{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macro-X-Ray-Fluorescence (MA-XRF) data typically do not fit into memory. In order to make optimal use of your computer memory and multi-core processors it is therefore necessary to convert such spectral data into a chunked analysis ready file format. The `maxrf4u` package makes use of a specially developed `.datastack` file format. \n",
    "\n",
    "In all of the data processing below we will store both the raw data and the results of further computations on disk in a `.datastack` file. The file format is based on the `ZipStore` file format of the [zarr](https://zarr.readthedocs.io/en/stable/tutorial.html#storage-alternatives) python package.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a .datastack file to do out-of-memory calculations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example below we show how to convert a MA-XRF dataset  consisting of a `.raw` data cube file (and an `.rpl` shape file) into a `.datastack` file. During conversion the `maxrf4u.raw_to_datastack()` function performs a Gaussian smoothing of each spectrum. Also the max and sum spectra are computed and stored. \n",
    "\n",
    "On my laptop this initial conversion of a 21 Gb dataset takes 7:11 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "os.chdir('/home/frank/Work/Projecten/DoRe/viz/raw_nmf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 49G\r\n",
      "-rw-rw-r-- 1 frank frank 91M okt 14 17:43 gridbox.html\r\n",
      "-rw-rw-r-- 1 frank frank 15M mrt 15 13:51 plot.svg\r\n",
      "-rw-rw-r-- 1 frank frank 29G mrt 22 13:42 RP-T-1898-A-3689.datastack\r\n",
      "-rwxrwxrwx 1 frank frank 21G mrt  9  2021 RP-T-1898-A-3689.raw\r\n",
      "-rwxrwxrwx 1 frank frank 181 mrt  9  2021 RP-T-1898-A-3689.rpl\r\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxrf4u import raw_to_datastack, tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing: RP-T-1898-A-3689.datastack...\n",
      "[########################################] | 100% Completed |  4min 21.8s\n",
      "Computing max spectrum...\n",
      "[########################################] | 100% Completed |  1min 18.6s\n",
      "Computing sum spectrum...\n",
      "[########################################] | 100% Completed |  1min 18.2s\n"
     ]
    }
   ],
   "source": [
    "raw_to_datastack('RP-T-1898-A-3689.raw', 'RP-T-1898-A-3689.rpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to check the content (i.e. the datasets) of the .datastack file with the `tree()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP-T-1898-A-3689.datastack:\n",
      "\n",
      "/\n",
      " ├── maxrf_cube (1692, 1592, 4096) float32\n",
      " ├── maxrf_maxspectrum (4096,) float32\n",
      " └── maxrf_sumspectrum (4096,) float64\n"
     ]
    }
   ],
   "source": [
    "tree('RP-T-1898-A-3689.datastack')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `show_arrays=True` option the `tree()` function also shows the shapes and chunk sizes of the individual datasets. For example, the `maxrf_cube` dataset contains `1692 x 1592` spectra with `4096` channels each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RP-T-1898-A-3689.datastack:\n",
      "\n",
      "/\n",
      " ├── maxrf_cube (1692, 1592, 4096) float32\n",
      " ├── maxrf_maxspectrum (4096,) float32\n",
      " └── maxrf_sumspectrum (4096,) float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "- Dataset: <h style=\"color:brown\">maxrf_cube</h><table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 44.13 GB </td> <td> 114.93 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (1692, 1592, 4096) </td> <td> (282, 398, 256) </td></tr>\n",
       "    <tr><th> Count </th><td> 385 Tasks </td><td> 384 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float32 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"209\" height=\"125\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"39\" y2=\"29\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"11\" x2=\"39\" y2=\"40\" />\n",
       "  <line x1=\"10\" y1=\"23\" x2=\"39\" y2=\"52\" />\n",
       "  <line x1=\"10\" y1=\"34\" x2=\"39\" y2=\"64\" />\n",
       "  <line x1=\"10\" y1=\"46\" x2=\"39\" y2=\"75\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"46\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"14\" y2=\"51\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"19\" y2=\"56\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"61\" />\n",
       "  <line x1=\"29\" y1=\"19\" x2=\"29\" y2=\"66\" />\n",
       "  <line x1=\"34\" y1=\"24\" x2=\"34\" y2=\"70\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"39\" y2=\"75\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.000000,0.000000 39.159007,29.159007 39.159007,75.799632 10.000000,46.640625\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"134\" y2=\"4\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"139\" y2=\"9\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"144\" y2=\"14\" />\n",
       "  <line x1=\"29\" y1=\"19\" x2=\"149\" y2=\"19\" />\n",
       "  <line x1=\"34\" y1=\"24\" x2=\"154\" y2=\"24\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"159\" y2=\"29\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"39\" y2=\"29\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"17\" y1=\"0\" x2=\"46\" y2=\"29\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"54\" y2=\"29\" />\n",
       "  <line x1=\"32\" y1=\"0\" x2=\"61\" y2=\"29\" />\n",
       "  <line x1=\"40\" y1=\"0\" x2=\"69\" y2=\"29\" />\n",
       "  <line x1=\"47\" y1=\"0\" x2=\"76\" y2=\"29\" />\n",
       "  <line x1=\"55\" y1=\"0\" x2=\"84\" y2=\"29\" />\n",
       "  <line x1=\"62\" y1=\"0\" x2=\"91\" y2=\"29\" />\n",
       "  <line x1=\"70\" y1=\"0\" x2=\"99\" y2=\"29\" />\n",
       "  <line x1=\"77\" y1=\"0\" x2=\"106\" y2=\"29\" />\n",
       "  <line x1=\"85\" y1=\"0\" x2=\"114\" y2=\"29\" />\n",
       "  <line x1=\"92\" y1=\"0\" x2=\"121\" y2=\"29\" />\n",
       "  <line x1=\"100\" y1=\"0\" x2=\"129\" y2=\"29\" />\n",
       "  <line x1=\"107\" y1=\"0\" x2=\"136\" y2=\"29\" />\n",
       "  <line x1=\"115\" y1=\"0\" x2=\"144\" y2=\"29\" />\n",
       "  <line x1=\"122\" y1=\"0\" x2=\"151\" y2=\"29\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"159\" y2=\"29\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.000000,0.000000 130.000000,0.000000 159.159007,29.159007 39.159007,29.159007\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"159\" y2=\"29\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"39\" y1=\"40\" x2=\"159\" y2=\"40\" />\n",
       "  <line x1=\"39\" y1=\"52\" x2=\"159\" y2=\"52\" />\n",
       "  <line x1=\"39\" y1=\"64\" x2=\"159\" y2=\"64\" />\n",
       "  <line x1=\"39\" y1=\"75\" x2=\"159\" y2=\"75\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"39\" y2=\"75\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"46\" y1=\"29\" x2=\"46\" y2=\"75\" />\n",
       "  <line x1=\"54\" y1=\"29\" x2=\"54\" y2=\"75\" />\n",
       "  <line x1=\"61\" y1=\"29\" x2=\"61\" y2=\"75\" />\n",
       "  <line x1=\"69\" y1=\"29\" x2=\"69\" y2=\"75\" />\n",
       "  <line x1=\"76\" y1=\"29\" x2=\"76\" y2=\"75\" />\n",
       "  <line x1=\"84\" y1=\"29\" x2=\"84\" y2=\"75\" />\n",
       "  <line x1=\"91\" y1=\"29\" x2=\"91\" y2=\"75\" />\n",
       "  <line x1=\"99\" y1=\"29\" x2=\"99\" y2=\"75\" />\n",
       "  <line x1=\"106\" y1=\"29\" x2=\"106\" y2=\"75\" />\n",
       "  <line x1=\"114\" y1=\"29\" x2=\"114\" y2=\"75\" />\n",
       "  <line x1=\"121\" y1=\"29\" x2=\"121\" y2=\"75\" />\n",
       "  <line x1=\"129\" y1=\"29\" x2=\"129\" y2=\"75\" />\n",
       "  <line x1=\"136\" y1=\"29\" x2=\"136\" y2=\"75\" />\n",
       "  <line x1=\"144\" y1=\"29\" x2=\"144\" y2=\"75\" />\n",
       "  <line x1=\"151\" y1=\"29\" x2=\"151\" y2=\"75\" />\n",
       "  <line x1=\"159\" y1=\"29\" x2=\"159\" y2=\"75\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"39.159007,29.159007 159.159007,29.159007 159.159007,75.799632 39.159007,75.799632\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"99.159007\" y=\"95.799632\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4096</text>\n",
       "  <text x=\"179.159007\" y=\"52.479320\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,179.159007,52.479320)\">1592</text>\n",
       "  <text x=\"14.579504\" y=\"81.220129\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,14.579504,81.220129)\">1692</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>- Dataset: <h style=\"color:brown\">maxrf_maxspectrum</h><table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 16.38 kB </td> <td> 16.38 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (4096,) </td> <td> (4096,) </td></tr>\n",
       "    <tr><th> Count </th><td> 2 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float32 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,25.412617 0.000000,25.412617\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4096</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>- Dataset: <h style=\"color:brown\">maxrf_sumspectrum</h><table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 32.77 kB </td> <td> 32.77 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (4096,) </td> <td> (4096,) </td></tr>\n",
       "    <tr><th> Count </th><td> 2 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,25.412617 0.000000,25.412617\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4096</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree('RP-T-1898-A-3689.datastack', show_arrays=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the freshly created `.datastack` file it is now possible to explore and process the data. Let's start this exploration by plotting numpy arrays that are returned by the `.read(<datapath>)` methods. The max and sum spectra provide a good summary of the spectral data. The horizontal axis shows the detector channel numbers. Note that inconveniently the data does not provide an energy calibration yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxrf4u import DataStack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataStack('RP-T-1898-A-3689.datastack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "..",
       "<img src=\"./images/10_storage_9091fd744c.png\">"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_max = ds.read('maxrf_maxspectrum')\n",
    "y_sum = ds.read('maxrf_sumspectrum')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[8, 5])\n",
    "ax2 = ax.twinx()\n",
    "ax.fill_between(np.arange(len(y_max)), y_max, color='r', alpha=0.3)\n",
    "ax2.plot(y_sum, color='g', alpha=0.7)\n",
    "ax.set_title('Smoothed XRF max and sum spectra')\n",
    "ax.set_xlabel('Channels [#]')\n",
    "ax.set_ylabel('Max spectrum Intensity [Counts] (red)')\n",
    "ax2.set_ylabel('Sum spectrum Intensity [Counts] (green)'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending and reading arrays and ragged lists \n",
    "\n",
    "In the code above a new `.datastack` file was created to store the converted MA-XRF data and further computations. If these computed results are shaped as a regular array one can use the `append()` function and `DataStack.read()` method for writing to and reading from the `.datastack` file. \n",
    "\n",
    "However, we will also come across situations in which we want to store and read irregular (ragged) lists of lists. In these cases one needs to make use of the `append_list()` function and the `DataStack.read_list()` method. \n",
    "\n",
    "Here is a small example to demonstrate this functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxrf4u import append_list, DataStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 5, 7], [1]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write \n",
    "my_test_ragged_list = [[1, 2], [3, 5, 7], [1]]\n",
    "append_list(my_test_ragged_list, 'test_list', 'RP-T-1898-A-3689.datastack')\n",
    "\n",
    "# and read \n",
    "ds = DataStack('RP-T-1898-A-3689.datastack')\n",
    "ds.read_list('test_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "\n",
    "import maxrf4u\n",
    "\n",
    "import numpy as np \n",
    "import dask \n",
    "import dask.array as da \n",
    "from dask.diagnostics import ProgressBar \n",
    "import dask_ndfilters \n",
    "import re \n",
    "import os \n",
    "import zarr \n",
    "from IPython.display import HTML \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.signal as ssg \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "\n",
    "# CONSTANTS \n",
    "DATASTACK_EXT = '.datastack' \n",
    "\n",
    "\n",
    "# COMPUTION ORDER \n",
    "\n",
    "class Layers: \n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        self.LAYERS = ['MAXRF_CUBE', \n",
    "                       'MAXRF_MAXSPECTRUM', \n",
    "                       'MAXRF_SUMSPECTRUM', \n",
    "                       'MAXRF_ENERGIES', \n",
    "                       'HOTMAX_PIXELS', \n",
    "                       'HOTMAX_SPECTRA', \n",
    "                       'HOTMAX_BASELINES', \n",
    "                       'HOTMAX_NOISELINES', \n",
    "                       'MAPS_IMVIS'] \n",
    "        \n",
    "        for l in self.LAYERS: \n",
    "            setattr(self, l, l.lower())\n",
    "\n",
    "L = Layers()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# functions \n",
    "\n",
    "def raw_to_datastack(raw_file, rpl_file, datastack_file=None, datapath=L.MAXRF_CUBE, verbose=True): \n",
    "    '''Convert Bruker Macro XRF (.raw) data file *raw_filename* and (.rpl) shape file *rpl_filename*.  \n",
    "    \n",
    "    into a Zarr Zipstore datastack file (.datastack).'''\n",
    "    \n",
    "    # generate datastack filename from .raw \n",
    "    if datastack_file is None: \n",
    "        datastack_file = re.sub('\\.raw$', DATASTACK_EXT, raw_file) \n",
    "        \n",
    "    # read data cube shape from .rpl file \n",
    "    with open(rpl_file, 'r') as fh: \n",
    "        lines = fh.readlines()\n",
    "\n",
    "    # get rid of spaces and newline characters \n",
    "    keys_and_values = dict([re.sub(' |\\n', '', l).split('\\t') for l in lines]) \n",
    "\n",
    "    width = int(keys_and_values['width'])\n",
    "    height = int(keys_and_values['height'])\n",
    "    depth = int(keys_and_values['depth'])\n",
    "\n",
    "    shape = (height, width, depth)\n",
    "    \n",
    "    # create numpy memory map \n",
    "    raw_mm = np.memmap(raw_file, dtype='uint16', mode='r', shape=(height, width, depth))[::-1, ::-1] \n",
    "\n",
    "    # initializing dask array \n",
    "    arr = da.from_array(raw_mm) \n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    # schedule spectral gaussian smoothing computation \n",
    "    smoothed = dask_ndfilters.gaussian_filter(arr, (0, 0, 7)) \n",
    "\n",
    "    # create and open an empty zip file\n",
    "    zs = zarr.ZipStore(datastack_file, mode='w') \n",
    "    \n",
    "    if verbose: \n",
    "        print(f'Writing: {datastack_file}...')\n",
    "\n",
    "    # compute and write maxrf data to zipstore \n",
    "    with ProgressBar(): \n",
    "        smoothed.to_zarr(zs, component=datapath) \n",
    "        \n",
    "    zs.close()\n",
    "    \n",
    "    # compute sum and max spectra and append to zipstore \n",
    "    \n",
    "    y_max, y_sum = max_and_sum_spectra(datastack_file, datapath=L.MAXRF_CUBE)\n",
    "    \n",
    "    append(y_max, L.MAXRF_MAXSPECTRUM, datastack_file)\n",
    "    append(y_sum, L.MAXRF_SUMSPECTRUM, datastack_file)\n",
    "    \n",
    "    \n",
    "\n",
    "def tree(datastack_file, show_arrays=False): \n",
    "    '''Prints content tree of *datastack_file* '''\n",
    "\n",
    "    with zarr.ZipStore(datastack_file, mode='r') as zs: \n",
    "        root = zarr.group(store=zs) \n",
    "        tree = root.tree(expand=True).__repr__()\n",
    "        print(f'{datastack_file}:\\n\\n{tree}')  \n",
    "        \n",
    "        if show_arrays:        \n",
    "            datasets = sorted(root)\n",
    "            arrays_html = ''\n",
    "\n",
    "            for ds in datasets: \n",
    "                arr = da.from_array(root[ds])\n",
    "                html = arr._repr_html_()\n",
    "                arrays_html = f'{arrays_html}- Dataset: <h style=\"color:brown\">{ds}</h>{html}' \n",
    "   \n",
    "            return HTML(arrays_html)\n",
    "\n",
    "def underscorify(datapath, datapath_list, extra_underscore=True): \n",
    "    '''Append extra underscore if *datapath* exists to prevent overwriting. \n",
    "    \n",
    "    If *extra_underscore=False* return (latest) datapath with most underscores'''\n",
    "    \n",
    "    if datapath in datapath_list: \n",
    "        r = re.compile(f'{datapath}_*$')\n",
    "        datapath = sorted(filter(r.match, datapath_list))[-1]\n",
    "        \n",
    "        if extra_underscore: \n",
    "            datapath = datapath + '_'\n",
    "        \n",
    "    return datapath \n",
    "\n",
    "\n",
    "def append(arr, datapath, datastack_file): \n",
    "    '''Add numpy or dask array *arr* to *datastack_file* in folder *datapath*.'''\n",
    "    \n",
    "    if not isinstance(arr, dask.array.Array):  \n",
    "        arr = da.from_array(arr) \n",
    "            \n",
    "    with zarr.ZipStore(datastack_file, mode='a') as zs: \n",
    "        root = zarr.group(store=zs)\n",
    "        \n",
    "        # append underscores to make unique if datapath exists \n",
    "        datapath_list = sorted(root) \n",
    "        datapath = underscorify(datapath, datapath_list)\n",
    "        \n",
    "        # write      \n",
    "        arr.to_zarr(zs, component=datapath)\n",
    "\n",
    "        \n",
    "def append_list(ragged_list, datapath, datastack_file, nan=-9999): \n",
    "    '''Wrapper around append() to store iregular (ragged) lists of lists as regular padded arrays.  \n",
    "    \n",
    "    Currently only working for two dimensional lists of integers. Padding is done with nan=-9999. \n",
    "    ''' \n",
    "    \n",
    "    padded_array = _straighten(ragged_list, nan=nan) \n",
    "        \n",
    "    append(padded_array, datapath, datastack_file) \n",
    "    \n",
    "\n",
    "def repack(datastack_file, select='all', overwrite=True, verbose=False): \n",
    "    '''Repack *datastack_file* by deleting and renaming all but latest datasets. \n",
    "    \n",
    "    Automatic selection of latest datasets can be overriden be providing list of *select* datasets''' \n",
    "    \n",
    "    if verbose: \n",
    "        tree(datastack_file)\n",
    "    \n",
    "    # open existing zipstore  \n",
    "    zs = zarr.ZipStore(datastack_file, mode='r') \n",
    "    root = zarr.group(store=zs)\n",
    "    datapath_list = sorted(root)  \n",
    "    \n",
    "    # select newest version (most underscores) for all datasets\n",
    "    if select == 'all': \n",
    "        selected = sorted(set([underscorify(dp, datapath_list, extra_underscore=False) for dp in datapath_list])) \n",
    "    # select newest version (most underscores) for datasets in select\n",
    "    else: \n",
    "        selected = sorted(set([underscorify(dp, datapath_list, extra_underscore=False) for dp in select]))       \n",
    "    \n",
    "    # remove underscores \n",
    "    renamed = [re.sub('_*$', '', s) for s in selected] \n",
    "    \n",
    "    # create and open new empty zipstore \n",
    "    datastack_file_new = datastack_file + '_temp'\n",
    "    zs_new = zarr.ZipStore(datastack_file_new, mode='w') \n",
    "    \n",
    "    # copy selected datasets into new zipstore \n",
    "    with ProgressBar(): \n",
    "        for src, dst in zip(selected, renamed): \n",
    "            print(f'Repacking dataset: \\'{src}\\'') \n",
    "            arr = da.from_array(root[src])\n",
    "            arr.to_zarr(zs_new, component=dst)\n",
    "    \n",
    "    zs.close()\n",
    "    zs_new.close()\n",
    "    \n",
    "    # finally overwrite old with new  \n",
    "    if overwrite: \n",
    "        os.replace(datastack_file_new, datastack_file)\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        tree(datastack_file)\n",
    "        \n",
    "\n",
    "def max_and_sum_spectra(datastack_file, datapath=L.MAXRF_CUBE): \n",
    "    '''Compute sum spectrum and max spectrum for 'maxrf' dataset in *datastack_file*. \n",
    "    \n",
    "    Returns: *y_sum*, *y_max*'''\n",
    "    \n",
    "    # open existing zipstore  \n",
    "    zs = zarr.ZipStore(datastack_file, mode='r') \n",
    "    root = zarr.group(store=zs)\n",
    "    \n",
    "    # initialize dask array \n",
    "    arr = da.from_array(root[datapath])\n",
    "        \n",
    "    # flatten\n",
    "    h, w, d = arr.shape\n",
    "    arr_flat = arr.reshape([h*w, d]) \n",
    "    \n",
    "    # schedule computations \n",
    "    sum_spectrum = arr_flat.sum(axis=0)\n",
    "    max_spectrum = arr_flat.max(axis=0)\n",
    "    \n",
    "    # compute \n",
    "    print('Computing max spectrum...')\n",
    "    with ProgressBar():\n",
    "        y_max = max_spectrum.compute() \n",
    "    print('Computing sum spectrum...')\n",
    "    with ProgressBar(): \n",
    "        y_sum = sum_spectrum.compute() / (h * w)\n",
    "        \n",
    "    zs.close()\n",
    "     \n",
    "    return y_max, y_sum \n",
    "\n",
    "\n",
    "        \n",
    "class DataStack: \n",
    "        \n",
    "    def __init__(self, datastack_file, mode='r', verbose=False, show_arrays=True): \n",
    "        '''Initialize DataStack object from *datastack_file*.''' \n",
    "        \n",
    "        # default computation layers ordering as attributes  \n",
    "        \n",
    "        self.LAYERS = L.LAYERS \n",
    "        \n",
    "        for l in L.LAYERS: \n",
    "            setattr(self, l, l.lower())\n",
    "            \n",
    "        # read datasets from file  \n",
    "        \n",
    "        self.mode = mode \n",
    "        self.datastack_file = datastack_file \n",
    "        \n",
    "        self.update_attrs()\n",
    "            \n",
    "        # print tree \n",
    "        if verbose: \n",
    "            tree(self.datastack_file, show_arrays=show_arrays) \n",
    "            \n",
    "    def update_attrs(self): \n",
    "        \n",
    "        # populate store attributes \n",
    "        self.store = zarr.ZipStore(self.datastack_file, mode=self.mode) \n",
    "        self.root = zarr.group(store=self.store) \n",
    "        \n",
    "        # generic exposure to dask arrays \n",
    "        self.datapath_list = sorted(self.root) \n",
    "        self.datasets = {dp: da.from_array(self.root[dp]) for dp in self.datapath_list}\n",
    "        \n",
    "        # attributify dask arrays \n",
    "        # useful for code development, perhaps confusing for users \n",
    "        # might turn off this feature later \n",
    "        for dp, ds in self.datasets.items(): \n",
    "            setattr(self, dp, ds) \n",
    "        \n",
    "            \n",
    "    def latest(self, datapath): \n",
    "        '''Return latest version of datapath. '''\n",
    "        \n",
    "        datapath = underscorify(datapath, self.datapath_list, extra_underscore=False)\n",
    "        \n",
    "        return datapath \n",
    "        \n",
    "            \n",
    "    def read(self, datapath, latest=True, compute=True):\n",
    "        '''Read latest version of dataset for *datapath*\n",
    "        \n",
    "        Returns numpy array if dataset exists. Otherwise exits. '''\n",
    "        \n",
    "        if datapath in self.datapath_list: \n",
    "            if latest: \n",
    "                datapath = self.latest(datapath)     \n",
    "            dataset = self.datasets[datapath] \n",
    "            if compute: \n",
    "                dataset = dataset.compute()\n",
    "                \n",
    "        # no dataset in file        \n",
    "        else: \n",
    "            dataset = None \n",
    "            \n",
    "            self.tree()\n",
    "            assert False, f'Dataset not found: {datapath}'\n",
    "    \n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def read_list(self, datapath, latest=True, nan=-9999): \n",
    "        '''Thin wrapper for reading padded arrays (ragged lists). \n",
    "\n",
    "        Returns ragged list if dataset exists. Current implementation only for \n",
    "        two-dimensional (ragged) list of lists. ''' \n",
    "\n",
    "        # step 1: read (padded array \n",
    "        padded_array = self.read(datapath, latest=latest, compute=True)\n",
    "\n",
    "        # step 2: convert to ragged list by removing nan values \n",
    "        ragged_list = _unstraighten(padded_array, nan=nan)\n",
    "\n",
    "        return ragged_list \n",
    "\n",
    "\n",
    "    \n",
    "    def tree(self, show_arrays=False): \n",
    "        '''Prints content tree of datastack.'''\n",
    "        \n",
    "        tree(self.datastack_file, show_arrays=show_arrays)\n",
    "            \n",
    "\n",
    "            \n",
    "    def close(self): \n",
    "        '''Close file handle'''\n",
    "         \n",
    "        self.store.close()\n",
    "        self.mode = 'closed' \n",
    "        \n",
    "        print(f'Closed: {self.datastack_file}')             \n",
    "\n",
    "def _straighten(ragged_list, nan=-9999): \n",
    "    '''Utility function to straighten a `ragged_list` of integers indices into a regular (padded) array. \n",
    "    \n",
    "    \n",
    "    Creates a two dimensional numpy array with empty values padded with nan=-9999. \n",
    "    \n",
    "    Returns: padded_array \n",
    "    '''\n",
    "    \n",
    "    # determine shape \n",
    "    ncols = max([len(idxs) for idxs in ragged_list])\n",
    "    nrows = len(ragged_list) \n",
    "    \n",
    "    # initialize \n",
    "    padded_array = np.ones([nrows, ncols], dtype=int)\n",
    "    padded_array[:,:] = nan \n",
    "    \n",
    "    # fill \n",
    "    \n",
    "    for i, indices in enumerate(ragged_list): \n",
    "        for j, idx in enumerate(indices): \n",
    "            padded_array[i, j] = idx \n",
    "        \n",
    "    return padded_array \n",
    "    \n",
    "\n",
    "def _unstraighten(padded_array, nan=-9999):\n",
    "    '''Convert a numpy `padded_array` of integers filled out with nan's into a ragged list.\n",
    "    \n",
    "    \n",
    "    Returns: a ragged list of lists \n",
    "    '''\n",
    "\n",
    "    ragged_list = []\n",
    "    \n",
    "    for row in padded_array: \n",
    "        row_list = list(row[row!=nan]) # remove nan's from list  \n",
    "        ragged_list.append(row_list)\n",
    "\n",
    "    return ragged_list\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
